{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792afa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mberronesreyes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mberronesreyes/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mberronesreyes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/mberronesreyes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mberronesreyes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk pandas\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37320838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"dreamlike_prompts.csv\")\n",
    "\n",
    "\n",
    "def get_random_prompt(length):\n",
    "    if length == \"short\":\n",
    "        col = \"short_prompt\"\n",
    "    elif length == \"mid\":\n",
    "        col = \"mid_prompt\"\n",
    "    elif length == \"long\":\n",
    "        col = \"long_prompt\"\n",
    "    else:\n",
    "        raise ValueError(\"Length must be 'short', 'mid', or 'long'.\")\n",
    "    \n",
    "    prompt = df[col].dropna().sample(1).values[0]\n",
    "    return prompt\n",
    "\n",
    "def extract_keywords(prompt, max_words=5):\n",
    "    words = nltk.word_tokenize(prompt)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    # POS categories \n",
    "    #https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n",
    "    categories = {\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"VERB\": [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\"]\n",
    "    }\n",
    "\n",
    "    # Store candidates per category\n",
    "    candidates = {cat: [] for cat in categories}\n",
    "\n",
    "    for w, pos in tagged:\n",
    "        w_clean = w.lower()\n",
    "        if w_clean.isalpha() and w_clean not in stop_words:  # only keep words, no punctuation or fillers\n",
    "            for cat, tags in categories.items():\n",
    "                if pos in tags:\n",
    "                    candidates[cat].append(w_clean)\n",
    "\n",
    "    # Prioritize: NOUNS first, then VERBS, then ADJ/ADV\n",
    "    chosen = []\n",
    "    for cat in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
    "        if candidates[cat]:\n",
    "            chosen.append(random.choice(candidates[cat]))\n",
    "        if len(chosen) >= max_words:\n",
    "            break\n",
    "\n",
    "    # If fewer than 3 found, pad with additional randoms from any category\n",
    "    all_candidates = sum(candidates.values(), [])\n",
    "    while len(chosen) < 3 and all_candidates:\n",
    "        extra = random.choice(all_candidates)\n",
    "        if extra not in chosen:\n",
    "            chosen.append(extra)\n",
    "\n",
    "    return chosen\n",
    "\n",
    "def find_prompt_with_words(words, df, min_matches=2):\n",
    "    for prompt in df[\"long_prompt\"].dropna():\n",
    "        count = sum(1 for w in words if w.lower() in prompt.lower())\n",
    "        if count >= min_matches:\n",
    "            return prompt\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2e677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected prompt: The path was covered in mirrors, and each reflection was of a different version of me.\n",
      "Extracted keywords: ['path', 'covered', 'different']\n",
      "Matching prompt: The ground beneath me was covered in sand, but the grains sparkled like tiny mirrors, each one reflecting a different version of myself.\n"
     ]
    }
   ],
   "source": [
    "# Pick short, mid, long\n",
    "prompt = get_random_prompt(\"mid\")\n",
    "print(\"Selected prompt:\", prompt)\n",
    "\n",
    "keywords = extract_keywords(prompt)\n",
    "print(\"Extracted keywords:\", keywords)\n",
    "\n",
    "match = find_prompt_with_words(keywords, df)\n",
    "print(\"Matching prompt:\", match if match else \"No match found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
